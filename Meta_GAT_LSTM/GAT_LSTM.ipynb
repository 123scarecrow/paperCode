{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gat import GAT \n",
    "import process\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BasicModel:\n",
    "    def __init__(self, dim_input, dim_output, seq_length,\n",
    "                 filter_num, dim_cnn_flatten, dim_fc, dim_lstm_hidden,\n",
    "                 update_lr, meta_lr, meta_batch_size, update_batch_size,\n",
    "                 test_num_updates):\n",
    "        \"\"\" must call construct_model() after initializing MAML! \"\"\"\n",
    "        self.dim_input = dim_input\n",
    "        self.channels = dim_output\n",
    "        self.img_size = int(np.sqrt(self.dim_input / self.channels))\n",
    "\n",
    "        self.dim_output = dim_output\n",
    "        self.seq_length = seq_length\n",
    "        self.filter_num = filter_num\n",
    "        self.dim_cnn_flatten = dim_cnn_flatten\n",
    "        self.dim_fc = dim_fc\n",
    "        self.dim_lstm_hidden = dim_lstm_hidden\n",
    "\n",
    "        self.update_lr = update_lr\n",
    "        self.meta_lr = meta_lr\n",
    "        self.update_batch_size = update_batch_size\n",
    "        self.test_num_updates = test_num_updates\n",
    "\n",
    "        self.meta_batch_size = meta_batch_size\n",
    "\n",
    "        self.inputa = tf.placeholder(tf.float32)\n",
    "        self.inputb = tf.placeholder(tf.float32)\n",
    "        self.labela = tf.placeholder(tf.float32)\n",
    "        self.labelb = tf.placeholder(tf.float32)\n",
    "\n",
    "    def update(self, loss, weights):\n",
    "        grads = tf.gradients(loss, list(weights.values()))\n",
    "        gradients = dict(zip(weights.keys(), grads))\n",
    "        new_weights = dict(\n",
    "            zip(weights.keys(), [weights[key] - self.update_lr * gradients[key] for key in weights.keys()]))\n",
    "        return new_weights\n",
    "\n",
    "    def construct_convlstm(self):\n",
    "        weights = {}\n",
    "        dtype = tf.float32\n",
    "        conv_initializer = tf.contrib.layers.xavier_initializer_conv2d(dtype=dtype)\n",
    "        k = 3\n",
    "\n",
    "        weights['conv1'] = tf.get_variable('conv1', [k, k, self.channels, self.filter_num],\n",
    "                                           initializer=conv_initializer, dtype=dtype)\n",
    "        weights['b_conv1'] = tf.Variable(tf.zeros([self.filter_num]))\n",
    "\n",
    "        weights['conv2'] = tf.get_variable('conv2', [k, k, self.filter_num, self.filter_num],\n",
    "                                           initializer=conv_initializer, dtype=dtype)\n",
    "        weights['b_conv2'] = tf.Variable(tf.zeros([self.filter_num]))\n",
    "\n",
    "        weights['conv3'] = tf.get_variable('conv3', [k, k, self.filter_num, self.filter_num],\n",
    "                                           initializer=conv_initializer, dtype=dtype)\n",
    "        weights['b_conv3'] = tf.Variable(tf.zeros([self.filter_num]))\n",
    "\n",
    "        weights['fc1'] = tf.Variable(tf.random_normal([self.dim_cnn_flatten, self.dim_fc]), name='fc1')\n",
    "        weights['b_fc1'] = tf.Variable(tf.zeros([self.dim_fc]))\n",
    "\n",
    "        weights['kernel_lstm'] = tf.get_variable('kernel_lstm', [self.dim_fc + self.dim_lstm_hidden,\n",
    "                                                                 4 * self.dim_lstm_hidden])\n",
    "        weights['b_lstm'] = tf.Variable(tf.zeros([4 * self.dim_lstm_hidden]))\n",
    "\n",
    "        weights['b_fc2'] = tf.Variable(tf.zeros([self.dim_output]))\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def lstm(self, inp, weights):\n",
    "        def lstm_block(linp, pre_state, kweight, bweight, activation):\n",
    "            sigmoid = math_ops.sigmoid\n",
    "            one = constant_op.constant(1, dtype=dtypes.int32)\n",
    "            c, h = pre_state\n",
    "\n",
    "            gate_inputs = math_ops.matmul(\n",
    "                array_ops.concat([linp, h], 1), kweight)\n",
    "            gate_inputs = nn_ops.bias_add(gate_inputs, bweight)\n",
    "\n",
    "            i, j, f, o = array_ops.split(\n",
    "                value=gate_inputs, num_or_size_splits=4, axis=one)\n",
    "\n",
    "            forget_bias_tensor = constant_op.constant(1.0, dtype=f.dtype)\n",
    "\n",
    "            add = math_ops.add\n",
    "            multiply = math_ops.multiply\n",
    "            new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))),\n",
    "                        multiply(sigmoid(i), activation(j)))\n",
    "            new_h = multiply(activation(new_c), sigmoid(o))\n",
    "\n",
    "            new_state = [new_c, new_h]\n",
    "            return new_h, new_state\n",
    "\n",
    "        # unstack对矩阵分解\n",
    "        # transpose多维矩阵转置 perm=[1,0,2] 例如：2*3*4 -> 3*2*4\n",
    "        inp = tf.unstack(tf.transpose(inp, perm=[1, 0, 2]))\n",
    "        state = [tf.zeros([self.update_batch_size, self.dim_lstm_hidden]),\n",
    "                 tf.zeros([self.update_batch_size, self.dim_lstm_hidden])]\n",
    "        output = None\n",
    "        for t in range(len(inp)):\n",
    "            output, state = lstm_block(inp[t], state,\n",
    "                                       weights['kernel_lstm'], weights['b_lstm'],\n",
    "                                       tf.nn.tanh)\n",
    "        return output\n",
    "\n",
    "    def forward_convlstm(self, inp, weights, feature_size, nb_nodes, is_train,\n",
    "                                attn_drop, ffd_drop,\n",
    "                                bias_mat=bias_in,\n",
    "                                hid_units=hid_units, n_heads=n_heads,\n",
    "                                residual=residual, activation=nonlinearity):\n",
    "        \n",
    "        inp = tf.reshape(inp, [-1, self.dim_input])\n",
    "\n",
    "        gat_outputs = GAT.inference(inp, feature_size, nb_nodes, is_train,\n",
    "                                attn_drop, ffd_drop,\n",
    "                                bias_mat=bias_in,\n",
    "                                hid_units=hid_units, n_heads=n_heads,\n",
    "                                residual=residual, activation=nonlinearity)\n",
    "        #gat_outputs(1,nodeNum,features)\n",
    "        \n",
    "        cnn_outputs = tf.reshape(cnn_outputs, [-1, self.seq_length, self.dim_fc])\n",
    "\n",
    "        lstm_outputs = self.lstm(cnn_outputs, weights)\n",
    "        return lstm_outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class STDN(BasicModel):\n",
    "    def __init__(self, dim_input, dim_output, seq_length,\n",
    "                 filter_num, dim_cnn_flatten, dim_fc, dim_lstm_hidden,\n",
    "                 update_lr, meta_lr, meta_batch_size, update_batch_size,\n",
    "                 test_num_updates):\n",
    "        print(\"Initializing STDN...\")\n",
    "        BasicModel.__init__(self, dim_input, dim_output, seq_length,\n",
    "                            filter_num, dim_cnn_flatten, dim_fc, dim_lstm_hidden,\n",
    "                            update_lr, meta_lr, meta_batch_size, update_batch_size,\n",
    "                            test_num_updates)\n",
    "\n",
    "    def loss_func(self, pred, label):\n",
    "        pred = tf.reshape(pred, [-1])\n",
    "        label = tf.reshape(label, [-1])\n",
    "        return tf.reduce_mean(tf.square(pred - label))\n",
    "\n",
    "    def construct_model(self):\n",
    "        with tf.variable_scope('model', reuse=None):\n",
    "            with tf.variable_scope('maml', reuse=None):\n",
    "                self.weights = weights = self.construct_convlstm()\n",
    "                weights['fc2'] = tf.Variable(tf.random_normal(\n",
    "                    [self.dim_lstm_hidden, self.dim_output]), name='fc6')   # output layer\n",
    "\n",
    "            num_updates = self.test_num_updates\n",
    "\n",
    "            def task_metalearn(inp):\n",
    "                \"\"\" Perform gradient descent for one task in the meta-batch. \"\"\"\n",
    "                inputa, inputb, labela, labelb = inp\n",
    "                task_outputbs, task_lossesb = [], []\n",
    "\n",
    "                task_outputa = self.forward(inputa, weights)  # only reuse on the first iter\n",
    "                task_lossa = self.loss_func(task_outputa, labela)\n",
    "\n",
    "                fast_weights = self.update(task_lossa, weights)\n",
    "\n",
    "                output = self.forward(inputb, fast_weights)\n",
    "                task_outputbs.append(output)\n",
    "                task_lossesb.append(self.loss_func(output, labelb))\n",
    "\n",
    "                for j in range(num_updates - 1):\n",
    "                    loss = self.loss_func(self.forward(inputa, fast_weights), labela)\n",
    "                    fast_weights = self.update(loss, fast_weights)\n",
    "\n",
    "                    output = self.forward(inputb, fast_weights)\n",
    "                    task_outputbs.append(output)\n",
    "                    task_lossesb.append(self.loss_func(output, labelb))\n",
    "\n",
    "                task_output = [task_outputa, task_outputbs, task_lossa, task_lossesb]\n",
    "                return task_output\n",
    "\n",
    "            out_dtype = [tf.float32, [tf.float32]*num_updates, tf.float32, [tf.float32]*num_updates]\n",
    "\n",
    "            inputs = (self.inputa, self.inputb, self.labela, self.labelb)\n",
    "            result = tf.map_fn(task_metalearn,\n",
    "                               elems=inputs,\n",
    "                               dtype=out_dtype,\n",
    "                               parallel_iterations=self.meta_batch_size)\n",
    "            outputas, outputbs, lossesa, lossesb = result\n",
    "\n",
    "        # Performance & Optimization\n",
    "        self.total_loss1 = total_loss1 = tf.reduce_sum(lossesa) / tf.to_float(self.meta_batch_size)\n",
    "        self.total_losses2 = total_losses2 = [tf.reduce_sum(lossesb[j]) / tf.to_float(self.meta_batch_size)\n",
    "                                              for j in range(num_updates)]\n",
    "        self.total_rmse1 = tf.sqrt(lossesa)\n",
    "        self.total_rmse2 = [tf.sqrt(total_losses2[j]) for j in range(num_updates)]\n",
    "\n",
    "        self.outputas, self.outputbs = outputas, outputbs\n",
    "        self.pretrain_op = tf.train.AdamOptimizer(self.meta_lr).minimize(total_loss1)\n",
    "        self.metatrain_op = tf.train.AdamOptimizer(self.meta_lr).minimize(total_losses2[num_updates-1])\n",
    "\n",
    "        maml_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"model/maml\")\n",
    "        self.finetune_op = tf.train.AdamOptimizer(self.meta_lr).minimize(total_loss1, var_list=maml_vars)\n",
    "\n",
    "    def forward(self, inp, weights):\n",
    "        convlstm_outputs = self.forward_convlstm(inp, weights)\n",
    "        preds = tf.nn.tanh(tf.matmul(convlstm_outputs, weights['fc2']) + weights['b_fc2'])\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
