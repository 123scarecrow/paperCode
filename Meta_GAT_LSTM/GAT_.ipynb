{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#layers.py\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1d = tf.layers.conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#注意力层\n",
    "def attn_head(seq, out_sz, bias_mat, activation, in_drop=0.0, coef_drop=0.0, residual=False):\n",
    "    with tf.name_scope('my_attn'):\n",
    "        if in_drop != 0.0:\n",
    "            #防止过拟合，保留seq中1.0 - in_drop个数，保留的数并变为1/1.0 - in_drop\n",
    "            seq = tf.nn.dropout(seq, 1.0 - in_drop)\n",
    "        #将原始节点特征 seq 进行变换得到了 seq_fts。这里，作者使用卷积核大小为 1 的 1D 卷积模拟投影变换，\n",
    "        # 投影变换后的维度为 out_sz。注意，这里投影矩阵 W是所有节点共享，所以 1D 卷积中的多个卷积核也是共享的。\n",
    "        #seq_fts 的大小为 [num_graph, num_node, out_sz]\n",
    "        print('【seq】',seq)\n",
    "        #(1, 2708, 1433)\n",
    "        seq_fts = tf.layers.conv1d(seq, out_sz, 1, use_bias=False) \n",
    "        print('【seq_fts】',seq_fts)\n",
    "        #(1, 2708, 8)\n",
    "        # simplest self-attention possible\n",
    "        # f_1 和 f_2 维度均为 [num_graph, num_node, 1]\n",
    "        #(1, 2708, 1)\n",
    "        f_1 = tf.layers.conv1d(seq_fts, 1, 1)  #节点投影\n",
    "        f_2 = tf.layers.conv1d(seq_fts, 1, 1)  #邻居投影\n",
    "\n",
    "        #将 f_2 转置之后与 f_1 叠加，通过广播得到的大小为 [num_graph, num_node, num_node] 的 logits\n",
    "        logits = f_1 + tf.transpose(f_2, [0, 2, 1])#注意力矩阵\n",
    "        #(1, 2708, 2708)\n",
    "        print('【logits】',logits)\n",
    "        #+biase_mat是为了对非邻居节点mask,归一化的注意力矩阵\n",
    "        #邻接矩阵的作用，把和中心节点没有链接的注意力因子mask掉\n",
    "        coefs = tf.nn.softmax(tf.nn.leaky_relu(logits) + bias_mat)\n",
    "\n",
    "        if coef_drop != 0.0:\n",
    "            coefs = tf.nn.dropout(coefs, 1.0 - coef_drop)\n",
    "        if in_drop != 0.0:\n",
    "            seq_fts = tf.nn.dropout(seq_fts, 1.0 - in_drop)\n",
    "         #将 mask 之后的注意力矩阵 coefs 与变换后的特征矩阵 seq_fts 相乘，\n",
    "         # 即可得到更新后的节点表示 vals。\n",
    "        #(1, 2708, 2708)*(1,2708,8)=(1, 2708, 8)\n",
    "        vals = tf.matmul(coefs, seq_fts)\n",
    "        ret = tf.contrib.layers.bias_add(vals)\n",
    "\n",
    "        # residual connection\n",
    "        if residual:\n",
    "            if seq.shape[-1] != ret.shape[-1]:\n",
    "                ret = ret + conv1d(seq, ret.shape[-1], 1) # activation\n",
    "            else:\n",
    "                ret = ret + seq\n",
    "\n",
    "        return activation(ret)  # activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BaseGAttN\n",
    "import tensorflow as tf\n",
    "\n",
    "class BaseGAttN:\n",
    "    def loss(logits, labels, nb_classes, class_weights):\n",
    "        sample_wts = tf.reduce_sum(tf.multiply(tf.one_hot(labels, nb_classes), class_weights), axis=-1)\n",
    "        xentropy = tf.multiply(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=labels, logits=logits), sample_wts)\n",
    "        return tf.reduce_mean(xentropy, name='xentropy_mean')\n",
    "\n",
    "    def training(loss, lr, l2_coef):\n",
    "        # weight decay\n",
    "        vars = tf.trainable_variables()\n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in vars if v.name not\n",
    "                           in ['bias', 'gamma', 'b', 'g', 'beta']]) * l2_coef\n",
    "\n",
    "        # optimizer\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "        # training op\n",
    "        train_op = opt.minimize(loss+lossL2)\n",
    "        \n",
    "        return train_op\n",
    "\n",
    "    def preshape(logits, labels, nb_classes):\n",
    "        new_sh_lab = [-1]\n",
    "        new_sh_log = [-1, nb_classes]\n",
    "        log_resh = tf.reshape(logits, new_sh_log)\n",
    "        lab_resh = tf.reshape(labels, new_sh_lab)\n",
    "        return log_resh, lab_resh\n",
    "\n",
    "    def confmat(logits, labels):\n",
    "        preds = tf.argmax(logits, axis=1)\n",
    "        return tf.confusion_matrix(labels, preds)\n",
    "\n",
    "##########################\n",
    "# Adapted from tkipf/gcn #\n",
    "##########################\n",
    "\n",
    "    def masked_softmax_cross_entropy(logits, labels, mask):\n",
    "        \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        loss *= mask\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    def masked_sigmoid_cross_entropy(logits, labels, mask):\n",
    "        \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "        labels = tf.cast(labels, dtype=tf.float32)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "        loss=tf.reduce_mean(loss,axis=1)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        loss *= mask\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    def masked_accuracy(logits, labels, mask):\n",
    "        \"\"\"Accuracy with masking.\"\"\"\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        accuracy_all *= mask\n",
    "        return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "    def micro_f1(logits, labels, mask):\n",
    "        \"\"\"Accuracy with masking.\"\"\"\n",
    "        predicted = tf.round(tf.nn.sigmoid(logits))\n",
    "\n",
    "        # Use integers to avoid any nasty FP behaviour\n",
    "        predicted = tf.cast(predicted, dtype=tf.int32)\n",
    "        labels = tf.cast(labels, dtype=tf.int32)\n",
    "        mask = tf.cast(mask, dtype=tf.int32)\n",
    "\n",
    "        # expand the mask so that broadcasting works ([nb_nodes, 1])\n",
    "        mask = tf.expand_dims(mask, -1)\n",
    "        \n",
    "        # Count true positives, true negatives, false positives and false negatives.\n",
    "        tp = tf.count_nonzero(predicted * labels * mask)\n",
    "        tn = tf.count_nonzero((predicted - 1) * (labels - 1) * mask)\n",
    "        fp = tf.count_nonzero(predicted * (labels - 1) * mask)\n",
    "        fn = tf.count_nonzero((predicted - 1) * labels * mask)\n",
    "\n",
    "        # Calculate accuracy, precision, recall and F1 score.\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "        fmeasure = tf.cast(fmeasure, tf.float32)\n",
    "        return fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#from utils import layers\n",
    "#from models.base_gattn import BaseGAttN\n",
    "\n",
    "class GAT(BaseGAttN):\n",
    "    def inference(inputs, nb_classes, nb_nodes, training, attn_drop, ffd_drop,\n",
    "            bias_mat, hid_units, n_heads, activation=tf.nn.elu, residual=False):\n",
    "        attns = []\n",
    "        #将多头输出连接在一起concat，有n个输入层\n",
    "        for _ in range(n_heads[0]):\n",
    "            attns.append(attn_head(inputs, bias_mat=bias_mat,\n",
    "                out_sz=hid_units[0], activation=activation,\n",
    "                in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        #(1, 2708, 64)\n",
    "        h_1 = tf.concat(attns, axis=-1)\n",
    "    \n",
    "        print('【hid_units】',len(hid_units))\n",
    "        for i in range(1, len(hid_units)):\n",
    "            h_old = h_1\n",
    "            attns = []\n",
    "            for _ in range(n_heads[i]):\n",
    "                print('【hid_units】',hid_units)\n",
    "                attns.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "                    out_sz=hid_units[i], activation=activation,\n",
    "                    in_drop=ffd_drop, coef_drop=attn_drop, residual=residual))\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "        out = []\n",
    "        print('【h_1】',h_1)\n",
    "        for i in range(n_heads[-1]):\n",
    "            out.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "                out_sz=nb_classes, activation=lambda x: x,\n",
    "                in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]\n",
    "    \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "\n",
    "def load_data(dataset_str): # {'pubmed', 'citeseer', 'cora'}\n",
    "    \"\"\"Load data.\"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "    print(len(y))\n",
    "    print(labels.shape[0])\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    print(adj.shape)\n",
    "    print(features.shape)\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def load_random_data(size):\n",
    "\n",
    "    adj = sp.random(size, size, density=0.002) # density similar to cora\n",
    "    features = sp.random(size, 1000, density=0.015)\n",
    "    int_labels = np.random.randint(7, size=(size))\n",
    "    labels = np.zeros((size, 7)) # Nx7\n",
    "    labels[np.arange(size), int_labels] = 1\n",
    "\n",
    "    train_mask = np.zeros((size,)).astype(bool)\n",
    "    train_mask[np.arange(size)[0:int(size/2)]] = 1\n",
    "\n",
    "    val_mask = np.zeros((size,)).astype(bool)\n",
    "    val_mask[np.arange(size)[int(size/2):]] = 1\n",
    "\n",
    "    test_mask = np.zeros((size,)).astype(bool)\n",
    "    test_mask[np.arange(size)[int(size/2):]] = 1\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "  \n",
    "    # sparse NxN, sparse NxF, norm NxC, ..., norm Nx1, ...\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def standardize_data(f, train_mask):\n",
    "    \"\"\"Standardize feature matrix and convert to tuple representation\"\"\"\n",
    "    # standardize data\n",
    "    f = f.todense()\n",
    "    mu = f[train_mask == True, :].mean(axis=0)\n",
    "    sigma = f[train_mask == True, :].std(axis=0)\n",
    "    f = f[:, np.squeeze(np.array(sigma > 0))]\n",
    "    mu = f[train_mask == True, :].mean(axis=0)\n",
    "    sigma = f[train_mask == True, :].std(axis=0)\n",
    "    f = (f - mu) / sigma\n",
    "    return f\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features.todense(), sparse_to_tuple(features)\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features.todense(), sparse_to_tuple(features)\n",
    "\n",
    "def adj_to_bias(adj, sizes, nhood=1):\n",
    "    nb_graphs = adj.shape[0]\n",
    "    mt = np.empty(adj.shape)\n",
    "    for g in range(nb_graphs):\n",
    "        mt[g] = np.eye(adj.shape[1])\n",
    "        for _ in range(nhood):\n",
    "            mt[g] = np.matmul(mt[g], (adj[g] + np.eye(adj.shape[1])))\n",
    "        for i in range(sizes[g]):\n",
    "            for j in range(sizes[g]):\n",
    "                if mt[g][i][j] > 0.0:\n",
    "                    mt[g][i][j] = 1.0\n",
    "    return -1e9 * (1.0 - mt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: cora\n",
      "----- Opt. hyperparams -----\n",
      "lr: 0.005\n",
      "l2_coef: 0.0005\n",
      "----- Archi. hyperparams -----\n",
      "nb. layers: 1\n",
      "nb. units per layer: [8]\n",
      "nb. attention heads: [8, 1]\n",
      "residual: False\n",
      "nonlinearity: <function elu at 0x000000B75CABE510>\n",
      "model: <class '__main__.GAT'>\n",
      "140\n",
      "2708\n",
      "(2708, 2708)\n",
      "(2708, 1433)\n",
      "[ True  True  True ... False False False]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\app2018\\Anaconda35\\lib\\site-packages\\scipy\\sparse\\lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "D:\\app2018\\Anaconda35\\lib\\site-packages\\scipy\\sparse\\lil.py:516: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n"
     ]
    }
   ],
   "source": [
    "dataset = 'cora'\n",
    "# training params\n",
    "batch_size = 1\n",
    "nb_epochs = 10\n",
    "patience = 100\n",
    "lr = 0.005  # learning rate\n",
    "l2_coef = 0.0005  # weight decay\n",
    "hid_units = [8] # numbers of hidden units per each attention head in each layer\n",
    "n_heads = [8, 1] # additional entry for the output layer\n",
    "residual = False\n",
    "nonlinearity = tf.nn.elu\n",
    "model = GAT\n",
    "\n",
    "print('Dataset: ' + dataset)\n",
    "print('----- Opt. hyperparams -----')\n",
    "print('lr: ' + str(lr))\n",
    "print('l2_coef: ' + str(l2_coef))\n",
    "print('----- Archi. hyperparams -----')\n",
    "print('nb. layers: ' + str(len(hid_units)))\n",
    "print('nb. units per layer: ' + str(hid_units))\n",
    "print('nb. attention heads: ' + str(n_heads))\n",
    "print('residual: ' + str(residual))\n",
    "print('nonlinearity: ' + str(nonlinearity))\n",
    "print('model: ' + str(model))\n",
    "\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(dataset)\n",
    "features, spars = preprocess_features(features)\n",
    "print(train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_nodes = features.shape[0]\n",
    "ft_size = features.shape[1]\n",
    "nb_classes = y_train.shape[1]\n",
    "\n",
    "adj = adj.todense()\n",
    "\n",
    "features = features[np.newaxis]\n",
    "adj = adj[np.newaxis]\n",
    "y_train = y_train[np.newaxis]\n",
    "y_val = y_val[np.newaxis]\n",
    "y_test = y_test[np.newaxis]\n",
    "train_mask = train_mask[np.newaxis]\n",
    "val_mask = val_mask[np.newaxis]\n",
    "test_mask = test_mask[np.newaxis]\n",
    "\n",
    "biases = adj_to_bias(adj, [nb_nodes], nhood=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【seq】 Tensor(\"my_attn/dropout/mul:0\", shape=(1, 2708, 1433), dtype=float32)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'seq_fts' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-af8d6e771382>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m                                 \u001b[0mbias_mat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias_in\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                                 \u001b[0mhid_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhid_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                                 residual=residual, activation=nonlinearity)\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'【logits】'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-3a679feea81d>\u001b[0m in \u001b[0;36minference\u001b[1;34m(inputs, nb_classes, nb_nodes, training, attn_drop, ffd_drop, bias_mat, hid_units, n_heads, activation, residual)\u001b[0m\n\u001b[0;32m     13\u001b[0m             attns.append(attn_head(inputs, bias_mat=bias_mat,\n\u001b[0;32m     14\u001b[0m                 \u001b[0mout_sz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhid_units\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;31m#(1, 2708, 64)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mh_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-381464557562>\u001b[0m in \u001b[0;36mattn_head\u001b[1;34m(seq, out_sz, bias_mat, activation, in_drop, coef_drop, residual)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# f_1 和 f_2 维度均为 [num_graph, num_node, 1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m#(1, 2708, 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mf_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_fts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#节点投影\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mf_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_fts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#邻居投影\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'seq_fts' referenced before assignment"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    with tf.name_scope('input'):\n",
    "        ftr_in = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_nodes, ft_size))\n",
    "        bias_in = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_nodes, nb_nodes))\n",
    "        lbl_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes, nb_classes))\n",
    "        msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes))\n",
    "        attn_drop = tf.placeholder(dtype=tf.float32, shape=())\n",
    "        ffd_drop = tf.placeholder(dtype=tf.float32, shape=())\n",
    "        is_train = tf.placeholder(dtype=tf.bool, shape=())\n",
    "\n",
    "    logits = model.inference(ftr_in, nb_classes, nb_nodes, is_train,\n",
    "                                attn_drop, ffd_drop,\n",
    "                                bias_mat=bias_in,\n",
    "                                hid_units=hid_units, n_heads=n_heads,\n",
    "                                residual=residual, activation=nonlinearity)\n",
    "    \n",
    "    print('【logits】',logits)\n",
    "    log_resh = tf.reshape(logits, [-1, nb_classes])\n",
    "    lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n",
    "    msk_resh = tf.reshape(msk_in, [-1])\n",
    "    loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n",
    "    accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n",
    "\n",
    "    train_op = model.training(loss, lr, l2_coef)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "    vlss_mn = np.inf\n",
    "    vacc_mx = 0.0\n",
    "    curr_step = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "\n",
    "        train_loss_avg = 0\n",
    "        train_acc_avg = 0\n",
    "        val_loss_avg = 0\n",
    "        val_acc_avg = 0\n",
    "\n",
    "        for epoch in range(nb_epochs):\n",
    "            tr_step = 0\n",
    "            tr_size = features.shape[0]\n",
    "            #\n",
    "            while tr_step * batch_size < tr_size:\n",
    "                _, loss_value_tr, acc_tr = sess.run([train_op, loss, accuracy],\n",
    "                    feed_dict={\n",
    "                        ftr_in: features[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                        bias_in: biases[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                        lbl_in: y_train[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                        msk_in: train_mask[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                        is_train: True,\n",
    "                        attn_drop: 0.6, ffd_drop: 0.6})\n",
    "                train_loss_avg += loss_value_tr\n",
    "                train_acc_avg += acc_tr\n",
    "                tr_step += 1\n",
    "\n",
    "            vl_step = 0\n",
    "            vl_size = features.shape[0]\n",
    "\n",
    "            while vl_step * batch_size < vl_size:\n",
    "                loss_value_vl, acc_vl = sess.run([loss, accuracy],\n",
    "                    feed_dict={\n",
    "                        ftr_in: features[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                        bias_in: biases[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                        lbl_in: y_val[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                        msk_in: val_mask[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                        is_train: False,\n",
    "                        attn_drop: 0.0, ffd_drop: 0.0})\n",
    "                val_loss_avg += loss_value_vl\n",
    "                val_acc_avg += acc_vl\n",
    "                vl_step += 1\n",
    "\n",
    "            print('Training: loss = %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f' %\n",
    "                    (train_loss_avg/tr_step, train_acc_avg/tr_step,\n",
    "                    val_loss_avg/vl_step, val_acc_avg/vl_step))\n",
    "\n",
    "            if val_acc_avg/vl_step >= vacc_mx or val_loss_avg/vl_step <= vlss_mn:\n",
    "                if val_acc_avg/vl_step >= vacc_mx and val_loss_avg/vl_step <= vlss_mn:\n",
    "                    vacc_early_model = val_acc_avg/vl_step\n",
    "                    vlss_early_model = val_loss_avg/vl_step\n",
    "                    #saver.save(sess, checkpt_file)\n",
    "                vacc_mx = np.max((val_acc_avg/vl_step, vacc_mx))\n",
    "                vlss_mn = np.min((val_loss_avg/vl_step, vlss_mn))\n",
    "                curr_step = 0\n",
    "            else:\n",
    "                curr_step += 1\n",
    "                if curr_step == patience:\n",
    "                    print('Early stop! Min loss: ', vlss_mn, ', Max accuracy: ', vacc_mx)\n",
    "                    print('Early stop model validation loss: ', vlss_early_model, ', accuracy: ', vacc_early_model)\n",
    "                    break\n",
    "\n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "\n",
    "        #saver.restore(sess, checkpt_file)\n",
    "\n",
    "        ts_size = features.shape[0]\n",
    "        ts_step = 0\n",
    "        ts_loss = 0.0\n",
    "        ts_acc = 0.0\n",
    "\n",
    "        while ts_step * batch_size < ts_size:\n",
    "            loss_value_ts, acc_ts = sess.run([loss, accuracy],\n",
    "                feed_dict={\n",
    "                    ftr_in: features[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                    bias_in: biases[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                    lbl_in: y_test[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                    msk_in: test_mask[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                    is_train: False,\n",
    "                    attn_drop: 0.0, ffd_drop: 0.0})\n",
    "            ts_loss += loss_value_ts\n",
    "            ts_acc += acc_ts\n",
    "            ts_step += 1\n",
    "\n",
    "        print('Test loss:', ts_loss/ts_step, '; Test accuracy:', ts_acc/ts_step)\n",
    "\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,2]])\n",
    "b = np.array([[1,2],[2,2],[3,2]])\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4],\n",
       "       [2, 4],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "idx_train = range(10)\n",
    "mask = np.zeros(10)\n",
    "print(mask)\n",
    "mask[idx_train] = 1\n",
    "np.array(mask, dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
